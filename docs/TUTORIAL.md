# BumbleCore ä¸‰é˜¶æ®µè®­ç»ƒå®éªŒæ•™ç¨‹

æœ¬æ•™ç¨‹å°†å¸¦ä½ ä»é›¶å¼€å§‹ï¼Œå®Œæ•´ä½“éªŒ**é¢„è®­ç»ƒ â†’ ç›‘ç£å¾®è°ƒ â†’ åå¥½ä¼˜åŒ–**ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œè®­ç»ƒä¸€ä¸ªå±äºä½ è‡ªå·±çš„ 1.5B å‚æ•°è¯­è¨€æ¨¡å‹ã€‚

## ğŸ¯ è®­ç»ƒæµç¨‹å›¾

```
é¢„è®­ç»ƒ (Pretraining)
â”œâ”€ è¾“å…¥ï¼šå¤§è§„æ¨¡æ–‡æœ¬æ•°æ® (1B tokens)
â”œâ”€ è¾“å‡ºï¼šåŸºåº§æ¨¡å‹ (Base Model)
â””â”€ ç›®æ ‡ï¼šå­¦ä¹ è¯­è¨€çš„åŸºç¡€çŸ¥è¯†å’Œè¡¨è¾¾èƒ½åŠ›
          â†“
ç›‘ç£å¾®è°ƒ (SFT)
â”œâ”€ è¾“å…¥ï¼šæŒ‡ä»¤-å›å¤å¯¹æ•°æ® (42K samples) + åŸºåº§æ¨¡å‹
â”œâ”€ è¾“å‡ºï¼šæŒ‡ä»¤æ¨¡å‹ (Instruct Model)
â””â”€ ç›®æ ‡ï¼šå­¦ä¹ éµå¾ªæŒ‡ä»¤å’Œå¯¹è¯èƒ½åŠ›
          â†“
åå¥½ä¼˜åŒ– (DPO)
â”œâ”€ è¾“å…¥ï¼šåå¥½å¯¹æ¯”æ•°æ® (10K samples) + æŒ‡ä»¤æ¨¡å‹
â”œâ”€ è¾“å‡ºï¼šå¯¹é½æ¨¡å‹ (Aligned Model)
â””â”€ ç›®æ ‡ï¼šå¯¹é½äººç±»åå¥½ï¼Œæå‡å›å¤è´¨é‡
```
> ğŸ’¡ **æç¤º**ï¼šä¸ºæ–¹ä¾¿è°ƒè¯•å’Œå¿«é€ŸéªŒè¯æµç¨‹ï¼Œæ‰€æœ‰è®­ç»ƒä¸æµ‹è¯•é˜¶æ®µæ‰€éœ€çš„**æœ€å°ç¤ºä¾‹æ•°æ®é›†**å‡å·²æä¾›åœ¨é¡¹ç›®ç›®å½•ä¸­ã€‚  
> ä½ å¯ä»¥ç›´æ¥ä½¿ç”¨ [`../datasets`](../datasets)ä¸­çš„æ ·æœ¬è¿›è¡Œç«¯åˆ°ç«¯æµ‹è¯•ã€‚

---

## ğŸš€ é˜¶æ®µä¸€ï¼šé¢„è®­ç»ƒï¼ˆPretrainingï¼‰

**å®éªŒç›®çš„**ï¼šä»éšæœºåˆå§‹åŒ–å¼€å§‹ï¼Œè®©æ¨¡å‹å­¦ä¹ è¯­è¨€çš„åŸºç¡€çŸ¥è¯†ã€‚

### 1ï¸âƒ£ å‡†å¤‡é¢„è®­ç»ƒæ•°æ®

æˆ‘ä»¬ä½¿ç”¨ ModelScope ä¸Šçš„ mini_pretrain_dataset æ•°æ®é›†ï¼ŒåŒ…å«çº¦ **1B tokens** çš„ä¸­æ–‡é¢„è®­ç»ƒæ•°æ®ã€‚

**æ•°æ®é›†åœ°å€**ï¼š[https://www.modelscope.cn/datasets/BazingaLyn/mini_pretrain_dataset/files](https://www.modelscope.cn/datasets/BazingaLyn/mini_pretrain_dataset/files)


**è¾“å…¥æ•°æ®æ ¼å¼ç¤ºä¾‹**ï¼š

```json
{"text": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨..."}
{"text": "æœºå™¨å­¦ä¹ æ˜¯å®ç°äººå·¥æ™ºèƒ½çš„ä¸€ç§æ–¹æ³•ï¼Œæ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é›†..."}
```

### 2ï¸âƒ£ é…ç½®è®­ç»ƒå‚æ•°

ç¼–è¾‘é¢„è®­ç»ƒè„šæœ¬ `scripts/pretrain.sh`ï¼š

**ä½¿ç”¨å‘½ä»¤è¡Œè¦†ç›– YAML ä¸­çš„å‚æ•°ï¼Œä¿®æ”¹æ•°æ®é›†åœ°å€**ï¼š

```bash
#!/bin/bash
deepspeed --include localhost:0,1 src/train.py \
    --yaml_config ./configs/pretrain/pretrain_full.yaml \
    --model_name_or_path ./models/bumblebee \
    --dataset_path <your dataset path> \
    --output_dir ./checkpoints/pretrain/bumblebee_1.5b_base
```

### 3ï¸âƒ£ å¯åŠ¨è®­ç»ƒ

```bash
bash scripts/pretrain.sh
```

### 4ï¸âƒ£ ç›‘æ§è®­ç»ƒè¿‡ç¨‹

è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½ å¯ä»¥ä½¿ç”¨ TensorBoard å®æ—¶ç›‘æ§è®­ç»ƒæŒ‡æ ‡ï¼š

```bash
# å¯åŠ¨ TensorBoardï¼ˆåœ¨æ–°ç»ˆç«¯ä¸­æ‰§è¡Œï¼‰
tensorboard --logdir=./checkpoints/pretrain/bumblebee_1.5b_base
```
> ğŸ’¡ **æ³¨æ„**ï¼šåç»­çš„ SFT å’Œ DPO é˜¶æ®µä¹Ÿå¯ä»¥ä½¿ç”¨ç›¸åŒçš„æ–¹å¼å¯åŠ¨ TensorBoard ç›‘æ§è®­ç»ƒè¿‡ç¨‹ï¼Œåªéœ€ä¿®æ”¹ `--logdir` å‚æ•°ä¸ºå¯¹åº”çš„è¾“å‡ºç›®å½•å³å¯ã€‚


**æœ€ç»ˆè®­ç»ƒæŸå¤±æ›²çº¿ï¼š**

![é¢„è®­ç»ƒæŸå¤±æ›²çº¿](../assets/train_loss/pretrain_training_loss.png)

---

## ğŸ“ é˜¶æ®µäºŒï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰

**å®éªŒç›®çš„**ï¼šè®©åŸºåº§æ¨¡å‹å­¦ä¼šç†è§£å’Œéµå¾ªäººç±»æŒ‡ä»¤ã€‚

### 1ï¸âƒ£ å‡†å¤‡ SFT æ•°æ®

æˆ‘ä»¬ä½¿ç”¨ LLaMA Factory æä¾›çš„é«˜è´¨é‡ä¸­æ–‡æŒ‡ä»¤æ•°æ®é›† alpaca_gpt4_zhï¼ŒåŒ…å« **42,677** æ¡æŒ‡ä»¤-å›å¤å¯¹ã€‚

**æ•°æ®é›†åœ°å€**ï¼š[https://huggingface.co/datasets/llamafactory/alpaca_gpt4_zh](https://huggingface.co/datasets/llamafactory/alpaca_gpt4_zh)

**è¾“å…¥æ•°æ®æ ¼å¼ç¤ºä¾‹**ï¼ˆAlpaca æ ¼å¼ï¼‰ï¼š

```json
{
  "instruction": "ä¿æŒå¥åº·çš„ä¸‰ä¸ªæç¤ºã€‚",
  "input": "",
  "output": "ä»¥ä¸‹æ˜¯ä¿æŒå¥åº·çš„ä¸‰ä¸ªæç¤ºï¼š\n1. ä¿æŒèº«ä½“æ´»åŠ¨ã€‚æ¯å¤©åšé€‚å½“çš„èº«ä½“è¿åŠ¨...\n2. å‡è¡¡é¥®é£Ÿã€‚æ¯å¤©é£Ÿç”¨æ–°é²œçš„è”¬èœã€æ°´æœ...\n3. ç¡çœ å……è¶³ã€‚ç¡çœ å¯¹äººä½“å¥åº·è‡³å…³é‡è¦..."
}
```

### 2ï¸âƒ£ é…ç½®è®­ç»ƒå‚æ•°

ç¼–è¾‘ SFT è„šæœ¬ `scripts/sft_full.sh`ï¼š

```bash
#!/bin/bash
deepspeed --include localhost:0,1 src/train.py \
    --yaml_config ./configs/sft/sft_full.yaml \
    --model_name_or_path ./checkpoints/pretrain/bumblebee_1.5b_base \
    --dataset_path <your dataset path> \
    --output_dir ./checkpoints/sft/bumblebee_1.5b_Instruct_full \
    --num_epochs 6.0
```

> ğŸ’¡ **é‡è¦**ï¼š`--model_name_or_path` ç°åœ¨æŒ‡å‘é˜¶æ®µä¸€çš„è¾“å‡ºï¼Œå®ç°æ¨¡å‹çš„è¿ç»­è®­ç»ƒ

### 3ï¸âƒ£ å¯åŠ¨è®­ç»ƒ

```bash
bash scripts/sft_full.sh
```

**æœ€ç»ˆè®­ç»ƒæŸå¤±æ›²çº¿ï¼š**

![SFT è®­ç»ƒæŸå¤±](../assets/train_loss/sft_training_loss.png)


---

## ğŸ¯ é˜¶æ®µä¸‰ï¼šåå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰

**å®éªŒç›®çš„**ï¼šé€šè¿‡äººç±»åå¥½æ•°æ®ï¼Œè®©æ¨¡å‹çš„å›å¤æ›´åŠ ä¼˜è´¨å’Œå®‰å…¨ã€‚

### 1ï¸âƒ£ å‡†å¤‡ DPO æ•°æ®

æˆ‘ä»¬ä½¿ç”¨ LLaMA Factory æä¾›çš„åŒè¯­åå¥½æ•°æ®é›† DPO-En-Zh-20k çš„**ä¸­æ–‡éƒ¨åˆ†**ï¼ŒåŒ…å«çº¦ **10,000** æ¡åå¥½å¯¹æ¯”æ•°æ®ã€‚

**æ•°æ®é›†åœ°å€**ï¼š[https://huggingface.co/datasets/llamafactory/DPO-En-Zh-20k](https://huggingface.co/datasets/llamafactory/DPO-En-Zh-20k)

**æ•°æ®æ ¼å¼ç¤ºä¾‹**ï¼ˆShareGPT æ ¼å¼ with chosen/rejectedï¼‰ï¼š

```json
{
  "conversations": [
    {
      "from": "human",
      "value": "ä»‹ç»ä¸€ä¸‹åŒ—äº¬"
    },
    {
      "from": "gpt",
      "value": "åŒ—äº¬æ˜¯ä¸­åäººæ°‘å…±å’Œå›½çš„é¦–éƒ½ï¼Œæœ‰ç€3000å¤šå¹´çš„å»ºåŸå²å’Œ860å¤šå¹´çš„å»ºéƒ½å²ã€‚ä½œä¸ºå…¨å›½çš„æ”¿æ²»ä¸­å¿ƒã€æ–‡åŒ–ä¸­å¿ƒå’Œå›½é™…äº¤å¾€ä¸­å¿ƒ..."
    }
  ],
  "chosen": {
    "from": "gpt",
    "value": "åŒ—äº¬æ˜¯ä¸­åäººæ°‘å…±å’Œå›½çš„é¦–éƒ½ï¼Œæœ‰ç€3000å¤šå¹´çš„å»ºåŸå²å’Œ860å¤šå¹´çš„å»ºéƒ½å²ã€‚ä½œä¸ºå…¨å›½çš„æ”¿æ²»ä¸­å¿ƒã€æ–‡åŒ–ä¸­å¿ƒå’Œå›½é™…äº¤å¾€ä¸­å¿ƒ..."
  },
  "rejected": {
    "from": "gpt",
    "value": "åŒ—äº¬æ˜¯ä¸ªåŸå¸‚ã€‚"
  }
}
```

### 2ï¸âƒ£ é…ç½®è®­ç»ƒå‚æ•°

ç¼–è¾‘ DPO è„šæœ¬ `scripts/dpo_lora.sh`ï¼š

```bash
#!/bin/bash
deepspeed --include localhost:0,1,2,3 src/train.py \
    --yaml_config ./configs/dpo/dpo_full.yaml \
    --model_name_or_path ./checkpoints/sft/bumblebee_1.5b_Instruct_full \
    --dataset_path <your dataset path> \
    --output_dir ./checkpoints/dpo/bumblebee_1.5b_dpo_lora
```

> ğŸ’¡ **é‡è¦**ï¼š`--model_name_or_path` æŒ‡å‘é˜¶æ®µäºŒçš„è¾“å‡ºï¼Œç»§ç»­ä¼˜åŒ–æ¨¡å‹

### 3ï¸âƒ£ å¯åŠ¨è®­ç»ƒ

```bash
bash scripts/dpo_lora.sh
```

**æœ€ç»ˆè®­ç»ƒæŸå¤±æ›²çº¿ï¼š**

![DPO è®­ç»ƒæŸå¤±](../assets/train_loss/dpo_training_loss.png)

**å‡†ç¡®ç‡æ›²çº¿ï¼š**

![DPO è®­ç»ƒå¥–åŠ±å’Œå‡†ç¡®ç‡](../assets/train_loss/dpo_training_rewards_accuracies.png)


### 4ï¸âƒ£ åˆå¹¶ LoRA æƒé‡

ç”±äº DPO é˜¶æ®µä½¿ç”¨äº† LoRA è®­ç»ƒï¼Œè®­ç»ƒåå¾—åˆ°çš„æ˜¯ LoRA é€‚é…å™¨æƒé‡ï¼Œéœ€è¦å°†å…¶åˆå¹¶åˆ°åŸºåº§æ¨¡å‹ä¸­æ‰èƒ½ä½¿ç”¨ã€‚

```bash
# ç¼–è¾‘åˆå¹¶è„šæœ¬
vim tools/run_merge_lora.sh

# é…ç½®ä»¥ä¸‹å‚æ•°ï¼š
# --base_model_path ./checkpoints/sft/bumblebee_1.5b_Instruct_full
# --lora_model_path ./checkpoints/dpo/bumblebee_1.5b_dpo_lora
# --save_path ./checkpoints/dpo/bumblebee_1.5b_dpo_merged

# æ‰§è¡Œåˆå¹¶
bash tools/run_merge_lora.sh
```

> ğŸ’¡ **æç¤º**ï¼šåˆå¹¶è¿‡ç¨‹ä¼šåŠ è½½åŸºåº§æ¨¡å‹å’Œ LoRA æƒé‡ï¼Œéœ€è¦ä¸€å®šçš„æ˜¾å­˜ã€‚åˆå¹¶å®Œæˆåï¼Œ`save_path` ç›®å½•å°†åŒ…å«å®Œæ•´çš„æ¨¡å‹æƒé‡å’Œåˆ†è¯å™¨ã€‚

---

### 5ï¸âƒ£ æµ‹è¯•æ¨¡å‹

è®­ç»ƒå®Œæˆåï¼Œå¯ä»¥æµ‹è¯•æ¨¡å‹çš„å¯¹è¯èƒ½åŠ›ï¼š

```bash
# ç¼–è¾‘ scripts/chat.shï¼Œè®¾ç½®æ¨¡å‹è·¯å¾„
vim scripts/chat.sh
# ä¿®æ”¹ä¸ºï¼š--model_path ./checkpoints/dpo/bumblebee_1.5b_dpo_merged

# å¯åŠ¨å¯¹è¯æµ‹è¯•
bash scripts/chat.sh
```