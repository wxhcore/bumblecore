import argparse
import os

import yaml

from .inference import BumblebeeChat, HFStreamChat

def start_chat_session(
    model_path,
    device_map,
    dtype,
    max_new_tokens,
    temperature,
    top_k,
    top_p,
    repetition_penalty,
    system_prompt,
    do_sample,
    enable_history,
    training_stage
):
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}ï¼Œè¯·ç¨å€™...")
    bot = BumblebeeChat(
        model_path=model_path,
        device_map=device_map,
        dtype=dtype
    )
    print("æ¨¡å‹åŠ è½½å®Œæˆï¼è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºèŠå¤©ã€‚\n")

    messages = []

    while True:
        user_input = input("ğŸ‘¤ ç”¨æˆ·: ").strip()
        if user_input.lower() in {"quit", "exit"}:
            print("ğŸ‘‹ å†è§ï¼")
            break

        if training_stage == "pretrain":
            print("ğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
            response_chunks = []
            for text in bot.stream_chat(
                messages=user_input,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                repetition_penalty=repetition_penalty,
                do_sample=do_sample
            ):
                print(text, end="", flush=True)
                response_chunks.append(text)
            print()
            continue

        if enable_history:
            messages.append({"role": "user", "content": user_input})
            current_messages = messages
        else:
            current_messages = [{"role": "user", "content": user_input}]

        print("ğŸ¤– åŠ©æ‰‹: ", end="", flush=True)

        response_chunks = []
        for text in bot.stream_chat(
            messages=current_messages,
            max_new_tokens=max_new_tokens,
            system_prompt=system_prompt,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            repetition_penalty=repetition_penalty,
            do_sample=do_sample
        ):
            print(text, end="", flush=True)
            response_chunks.append(text)
        print()

        full_response = "".join(response_chunks)

        if enable_history:
            messages.append({"role": "assistant", "content": full_response})

        print()


def bumblebee_streaming_chat():
    base_parser = argparse.ArgumentParser(add_help=False)
    base_parser.add_argument(
        "--yaml_config",
        type=str,
        default="",
        help="YAML é…ç½®æ–‡ä»¶è·¯å¾„",
    )
    cfg_args, _ = base_parser.parse_known_args()
    config_path = cfg_args.yaml_config
    
    if os.path.isfile(config_path):
        with open(config_path, "r", encoding="utf-8") as f:
            cfg = yaml.safe_load(f) or {}
    else:
        cfg = {}
    
    parser = argparse.ArgumentParser(
        description="å¯åŠ¨ Bumblebee èŠå¤©ä¼šè¯",
        parents=[base_parser],
    )
    parser.add_argument("--model_path", type=str, 
                        default=cfg.get("model_path"),
                        help="æ¨¡å‹è·¯å¾„")
                        
    parser.add_argument("--device_map", type=str, 
                        default=cfg.get("device_map", "auto"),
                        help="è®¾å¤‡æ˜ å°„ï¼ˆå¦‚ 'auto', 'cpu', 'cuda:0' ç­‰ï¼‰")

    parser.add_argument("--dtype", type=str, 
                        default=cfg.get("dtype", "auto"),
                        help="æ¨¡å‹æ•°æ®ç±»å‹ï¼ˆå¦‚ 'torch.float16', 'torch.bfloat16', 'auto'ï¼‰")

    parser.add_argument("--max_new_tokens", type=int, 
                        default=cfg.get("max_new_tokens", None),
                        help="æœ€å¤§ç”Ÿæˆ token æ•°")

    parser.add_argument("--system_prompt", type=str, 
                        default=cfg.get("system_prompt", None),
                        help="è®¾ç½® system promptï¼ˆä¸ä¼ ä½¿ç”¨é»˜è®¤ï¼‰")

    parser.add_argument("--temperature", type=float, 
                        default=cfg.get("temperature", None),
                        help="é‡‡æ ·æ¸©åº¦ï¼ˆä¸ä¼ ä½¿ç”¨æ¨¡å‹é»˜è®¤ï¼‰")

    parser.add_argument("--top_k", type=int, 
                        default=cfg.get("top_k", None),
                        help="Top_k é‡‡æ ·ï¼ˆä¸ä¼ ä½¿ç”¨æ¨¡å‹é»˜è®¤ï¼‰")

    parser.add_argument("--top_p", type=float, 
                        default=cfg.get("top_p", None),
                        help="Top_p (nucleus) é‡‡æ ·ï¼ˆä¸ä¼ ä½¿ç”¨æ¨¡å‹é»˜è®¤ï¼‰")

    parser.add_argument("--repetition_penalty", type=float, 
                        default=cfg.get("repetition_penalty", None),
                        help="é‡å¤æƒ©ç½šç³»æ•°ï¼ˆä¸ä¼ ä½¿ç”¨æ¨¡å‹é»˜è®¤ï¼‰")

    parser.add_argument("--do_sample", action="store_true",
                        default=cfg.get("do_sample", False),
                        help="å¯ç”¨é‡‡æ ·ï¼ˆå¦åˆ™ä½¿ç”¨è´ªå©ªè§£ç ï¼‰")

    parser.add_argument("--enable_history", action="store_true",
                        default=cfg.get("enable_history", False),
                        help="å¯ç”¨å¤šè½®å¯¹è¯å†å²")
    
    parser.add_argument("--training_stage", type=str,
                        default=cfg.get("training_stage","sft"),
                        choices=["sft", "dpo", "pretrain"],
                        help="æ¨¡å‹è®­ç»ƒé˜¶æ®µï¼šsftï¼ˆæŒ‡ä»¤å¾®è°ƒï¼‰ã€dpoï¼ˆåå¥½ä¼˜åŒ–ï¼‰ã€pretrainï¼ˆé¢„è®­ç»ƒï¼‰ã€‚")

    args = parser.parse_args()
    args_dict = vars(args)
    args_dict.pop("yaml_config", None)

    start_chat_session(**args_dict)